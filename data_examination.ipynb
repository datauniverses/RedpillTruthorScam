{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age Gap', 'Education', 'Economic Similarity', 'Social Similarities',\n",
       "       'Cultural Similarities', 'Social Gap', 'Common Interests',\n",
       "       'Religion Compatibility', 'No of Children from Previous Marriage',\n",
       "       'Desire to Marry', 'Independency',\n",
       "       'Relationship with the Spouse Family', 'Trading in', 'Engagement Time',\n",
       "       'Love', 'Commitment', 'Mental Health', 'The Sense of Having Children',\n",
       "       'Previous Trading', 'Previous Marriage',\n",
       "       'The Proportion of Common Genes', 'Addiction', 'Loyalty',\n",
       "       'Height Ratio', 'Good Income', 'Self Confidence',\n",
       "       'Relation with Non-spouse Before Marriage',\n",
       "       'Spouse Confirmed by Family', 'Divorce in the Family of Grade 1',\n",
       "       'Start Socializing with the Opposite Sex Age ', 'Divorce Probability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv(\"Marriage_Divorce_DB.csv\")\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age Gap</th>\n",
       "      <th>Education</th>\n",
       "      <th>Economic Similarity</th>\n",
       "      <th>Social Similarities</th>\n",
       "      <th>Cultural Similarities</th>\n",
       "      <th>Social Gap</th>\n",
       "      <th>Common Interests</th>\n",
       "      <th>Religion Compatibility</th>\n",
       "      <th>No of Children from Previous Marriage</th>\n",
       "      <th>Desire to Marry</th>\n",
       "      <th>...</th>\n",
       "      <th>Addiction</th>\n",
       "      <th>Loyalty</th>\n",
       "      <th>Height Ratio</th>\n",
       "      <th>Good Income</th>\n",
       "      <th>Self Confidence</th>\n",
       "      <th>Relation with Non-spouse Before Marriage</th>\n",
       "      <th>Spouse Confirmed by Family</th>\n",
       "      <th>Divorce in the Family of Grade 1</th>\n",
       "      <th>Start Socializing with the Opposite Sex Age</th>\n",
       "      <th>Divorce Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.111633</td>\n",
       "      <td>1.915111</td>\n",
       "      <td>10.998678</td>\n",
       "      <td>76.456065</td>\n",
       "      <td>47.847460</td>\n",
       "      <td>50.317656</td>\n",
       "      <td>88.099898</td>\n",
       "      <td>83.738075</td>\n",
       "      <td>4.402822</td>\n",
       "      <td>22.868019</td>\n",
       "      <td>...</td>\n",
       "      <td>3.134119</td>\n",
       "      <td>49.648480</td>\n",
       "      <td>30.822948</td>\n",
       "      <td>94.499164</td>\n",
       "      <td>45.964824</td>\n",
       "      <td>2.032610</td>\n",
       "      <td>1.719332</td>\n",
       "      <td>2.262242</td>\n",
       "      <td>24.356772</td>\n",
       "      <td>2.760190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.355384</td>\n",
       "      <td>2.957842</td>\n",
       "      <td>82.138120</td>\n",
       "      <td>48.656031</td>\n",
       "      <td>30.188517</td>\n",
       "      <td>54.114612</td>\n",
       "      <td>57.020971</td>\n",
       "      <td>98.408133</td>\n",
       "      <td>4.367024</td>\n",
       "      <td>40.336843</td>\n",
       "      <td>...</td>\n",
       "      <td>2.067377</td>\n",
       "      <td>75.220699</td>\n",
       "      <td>68.268221</td>\n",
       "      <td>41.102605</td>\n",
       "      <td>65.387715</td>\n",
       "      <td>1.053402</td>\n",
       "      <td>1.456192</td>\n",
       "      <td>9.795998</td>\n",
       "      <td>19.667152</td>\n",
       "      <td>1.962979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.527365</td>\n",
       "      <td>2.772463</td>\n",
       "      <td>26.337826</td>\n",
       "      <td>59.356238</td>\n",
       "      <td>10.340252</td>\n",
       "      <td>76.595377</td>\n",
       "      <td>80.590985</td>\n",
       "      <td>41.743462</td>\n",
       "      <td>1.197120</td>\n",
       "      <td>45.941845</td>\n",
       "      <td>...</td>\n",
       "      <td>3.599095</td>\n",
       "      <td>22.551866</td>\n",
       "      <td>59.134874</td>\n",
       "      <td>23.053577</td>\n",
       "      <td>84.271897</td>\n",
       "      <td>8.268308</td>\n",
       "      <td>7.095241</td>\n",
       "      <td>9.986173</td>\n",
       "      <td>15.522517</td>\n",
       "      <td>2.858803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.203075</td>\n",
       "      <td>1.729242</td>\n",
       "      <td>66.956033</td>\n",
       "      <td>5.472612</td>\n",
       "      <td>1.003407</td>\n",
       "      <td>55.071435</td>\n",
       "      <td>99.718078</td>\n",
       "      <td>70.493011</td>\n",
       "      <td>3.392041</td>\n",
       "      <td>2.924863</td>\n",
       "      <td>...</td>\n",
       "      <td>1.549274</td>\n",
       "      <td>99.172136</td>\n",
       "      <td>40.984117</td>\n",
       "      <td>43.400040</td>\n",
       "      <td>96.081229</td>\n",
       "      <td>5.852371</td>\n",
       "      <td>6.570749</td>\n",
       "      <td>5.099396</td>\n",
       "      <td>34.665933</td>\n",
       "      <td>1.404621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.864962</td>\n",
       "      <td>4.370290</td>\n",
       "      <td>76.245035</td>\n",
       "      <td>26.797234</td>\n",
       "      <td>93.291581</td>\n",
       "      <td>73.736241</td>\n",
       "      <td>52.896199</td>\n",
       "      <td>11.729729</td>\n",
       "      <td>2.373553</td>\n",
       "      <td>89.851492</td>\n",
       "      <td>...</td>\n",
       "      <td>4.031738</td>\n",
       "      <td>21.629472</td>\n",
       "      <td>89.122381</td>\n",
       "      <td>51.615509</td>\n",
       "      <td>53.330824</td>\n",
       "      <td>9.717223</td>\n",
       "      <td>7.609152</td>\n",
       "      <td>1.294295</td>\n",
       "      <td>22.545763</td>\n",
       "      <td>1.318819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6.962987</td>\n",
       "      <td>4.290912</td>\n",
       "      <td>8.386214</td>\n",
       "      <td>62.291307</td>\n",
       "      <td>54.683196</td>\n",
       "      <td>96.368597</td>\n",
       "      <td>70.819993</td>\n",
       "      <td>34.046550</td>\n",
       "      <td>4.244249</td>\n",
       "      <td>88.883882</td>\n",
       "      <td>...</td>\n",
       "      <td>3.858440</td>\n",
       "      <td>38.208996</td>\n",
       "      <td>69.785850</td>\n",
       "      <td>4.424765</td>\n",
       "      <td>89.069231</td>\n",
       "      <td>1.310490</td>\n",
       "      <td>4.791609</td>\n",
       "      <td>7.237212</td>\n",
       "      <td>25.947024</td>\n",
       "      <td>1.508494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.782445</td>\n",
       "      <td>3.134205</td>\n",
       "      <td>59.812318</td>\n",
       "      <td>65.977429</td>\n",
       "      <td>75.489836</td>\n",
       "      <td>37.836755</td>\n",
       "      <td>90.947532</td>\n",
       "      <td>21.744616</td>\n",
       "      <td>4.117648</td>\n",
       "      <td>72.646174</td>\n",
       "      <td>...</td>\n",
       "      <td>2.230562</td>\n",
       "      <td>52.062020</td>\n",
       "      <td>36.438678</td>\n",
       "      <td>6.802079</td>\n",
       "      <td>68.304033</td>\n",
       "      <td>1.402042</td>\n",
       "      <td>1.393353</td>\n",
       "      <td>3.153041</td>\n",
       "      <td>36.659010</td>\n",
       "      <td>1.564502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4.181422</td>\n",
       "      <td>1.213900</td>\n",
       "      <td>36.880983</td>\n",
       "      <td>66.255955</td>\n",
       "      <td>75.764438</td>\n",
       "      <td>13.180621</td>\n",
       "      <td>84.961868</td>\n",
       "      <td>99.551777</td>\n",
       "      <td>1.404545</td>\n",
       "      <td>91.350428</td>\n",
       "      <td>...</td>\n",
       "      <td>4.513791</td>\n",
       "      <td>21.057973</td>\n",
       "      <td>42.934991</td>\n",
       "      <td>31.621211</td>\n",
       "      <td>87.202084</td>\n",
       "      <td>9.236842</td>\n",
       "      <td>9.636910</td>\n",
       "      <td>6.178748</td>\n",
       "      <td>16.550659</td>\n",
       "      <td>2.645653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3.487983</td>\n",
       "      <td>1.658356</td>\n",
       "      <td>74.237358</td>\n",
       "      <td>41.293409</td>\n",
       "      <td>12.489656</td>\n",
       "      <td>94.018496</td>\n",
       "      <td>76.700340</td>\n",
       "      <td>7.633928</td>\n",
       "      <td>3.089036</td>\n",
       "      <td>88.040777</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127180</td>\n",
       "      <td>78.614687</td>\n",
       "      <td>13.793834</td>\n",
       "      <td>76.446395</td>\n",
       "      <td>54.355077</td>\n",
       "      <td>2.529191</td>\n",
       "      <td>4.424587</td>\n",
       "      <td>3.828083</td>\n",
       "      <td>19.358255</td>\n",
       "      <td>1.754198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.818757</td>\n",
       "      <td>3.087757</td>\n",
       "      <td>56.908983</td>\n",
       "      <td>41.398349</td>\n",
       "      <td>35.715356</td>\n",
       "      <td>90.396363</td>\n",
       "      <td>63.087658</td>\n",
       "      <td>60.403364</td>\n",
       "      <td>2.192293</td>\n",
       "      <td>87.136958</td>\n",
       "      <td>...</td>\n",
       "      <td>2.840089</td>\n",
       "      <td>36.005010</td>\n",
       "      <td>59.398621</td>\n",
       "      <td>55.553006</td>\n",
       "      <td>86.318802</td>\n",
       "      <td>2.238560</td>\n",
       "      <td>5.450913</td>\n",
       "      <td>7.333027</td>\n",
       "      <td>30.717932</td>\n",
       "      <td>1.935506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age Gap  Education  Economic Similarity  Social Similarities  \\\n",
       "0   0.111633   1.915111            10.998678            76.456065   \n",
       "1   3.355384   2.957842            82.138120            48.656031   \n",
       "2   6.527365   2.772463            26.337826            59.356238   \n",
       "3   5.203075   1.729242            66.956033             5.472612   \n",
       "4   6.864962   4.370290            76.245035            26.797234   \n",
       "..       ...        ...                  ...                  ...   \n",
       "95  6.962987   4.290912             8.386214            62.291307   \n",
       "96  0.782445   3.134205            59.812318            65.977429   \n",
       "97  4.181422   1.213900            36.880983            66.255955   \n",
       "98  3.487983   1.658356            74.237358            41.293409   \n",
       "99  5.818757   3.087757            56.908983            41.398349   \n",
       "\n",
       "    Cultural Similarities  Social Gap  Common Interests  \\\n",
       "0               47.847460   50.317656         88.099898   \n",
       "1               30.188517   54.114612         57.020971   \n",
       "2               10.340252   76.595377         80.590985   \n",
       "3                1.003407   55.071435         99.718078   \n",
       "4               93.291581   73.736241         52.896199   \n",
       "..                    ...         ...               ...   \n",
       "95              54.683196   96.368597         70.819993   \n",
       "96              75.489836   37.836755         90.947532   \n",
       "97              75.764438   13.180621         84.961868   \n",
       "98              12.489656   94.018496         76.700340   \n",
       "99              35.715356   90.396363         63.087658   \n",
       "\n",
       "    Religion Compatibility  No of Children from Previous Marriage  \\\n",
       "0                83.738075                               4.402822   \n",
       "1                98.408133                               4.367024   \n",
       "2                41.743462                               1.197120   \n",
       "3                70.493011                               3.392041   \n",
       "4                11.729729                               2.373553   \n",
       "..                     ...                                    ...   \n",
       "95               34.046550                               4.244249   \n",
       "96               21.744616                               4.117648   \n",
       "97               99.551777                               1.404545   \n",
       "98                7.633928                               3.089036   \n",
       "99               60.403364                               2.192293   \n",
       "\n",
       "    Desire to Marry  ...  Addiction    Loyalty  Height Ratio  Good Income  \\\n",
       "0         22.868019  ...   3.134119  49.648480     30.822948    94.499164   \n",
       "1         40.336843  ...   2.067377  75.220699     68.268221    41.102605   \n",
       "2         45.941845  ...   3.599095  22.551866     59.134874    23.053577   \n",
       "3          2.924863  ...   1.549274  99.172136     40.984117    43.400040   \n",
       "4         89.851492  ...   4.031738  21.629472     89.122381    51.615509   \n",
       "..              ...  ...        ...        ...           ...          ...   \n",
       "95        88.883882  ...   3.858440  38.208996     69.785850     4.424765   \n",
       "96        72.646174  ...   2.230562  52.062020     36.438678     6.802079   \n",
       "97        91.350428  ...   4.513791  21.057973     42.934991    31.621211   \n",
       "98        88.040777  ...   1.127180  78.614687     13.793834    76.446395   \n",
       "99        87.136958  ...   2.840089  36.005010     59.398621    55.553006   \n",
       "\n",
       "    Self Confidence  Relation with Non-spouse Before Marriage  \\\n",
       "0         45.964824                                  2.032610   \n",
       "1         65.387715                                  1.053402   \n",
       "2         84.271897                                  8.268308   \n",
       "3         96.081229                                  5.852371   \n",
       "4         53.330824                                  9.717223   \n",
       "..              ...                                       ...   \n",
       "95        89.069231                                  1.310490   \n",
       "96        68.304033                                  1.402042   \n",
       "97        87.202084                                  9.236842   \n",
       "98        54.355077                                  2.529191   \n",
       "99        86.318802                                  2.238560   \n",
       "\n",
       "    Spouse Confirmed by Family  Divorce in the Family of Grade 1  \\\n",
       "0                     1.719332                          2.262242   \n",
       "1                     1.456192                          9.795998   \n",
       "2                     7.095241                          9.986173   \n",
       "3                     6.570749                          5.099396   \n",
       "4                     7.609152                          1.294295   \n",
       "..                         ...                               ...   \n",
       "95                    4.791609                          7.237212   \n",
       "96                    1.393353                          3.153041   \n",
       "97                    9.636910                          6.178748   \n",
       "98                    4.424587                          3.828083   \n",
       "99                    5.450913                          7.333027   \n",
       "\n",
       "    Start Socializing with the Opposite Sex Age   Divorce Probability  \n",
       "0                                      24.356772             2.760190  \n",
       "1                                      19.667152             1.962979  \n",
       "2                                      15.522517             2.858803  \n",
       "3                                      34.665933             1.404621  \n",
       "4                                      22.545763             1.318819  \n",
       "..                                           ...                  ...  \n",
       "95                                     25.947024             1.508494  \n",
       "96                                     36.659010             1.564502  \n",
       "97                                     16.550659             2.645653  \n",
       "98                                     19.358255             1.754198  \n",
       "99                                     30.717932             1.935506  \n",
       "\n",
       "[100 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X = dataset.iloc[:, :-1] \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X = scaler_X.fit_transform(X)\n",
    "\n",
    "\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset['Divorce Probability'].values\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "y = scaler_y.fit_transform(y.reshape(-1 , 1))\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.scatter(X, y)\n",
    "# plt.title(\"\")\n",
    "# plt.xlabel(\"x\")\n",
    "# plt.ylabel(\"y\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# corr_coeff, p_value = pearsonr(X, y)\n",
    "# print(corr_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direction colleration using ranking,  spearman and kendall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "# corr_coeff, p_value = spearmanr(X, y)\n",
    "# print(corr_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import kendalltau\n",
    "\n",
    "# corr_coeff, p_value = kendalltau(X, y)\n",
    "# print(corr_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colleration messureed by the frequency mapped in information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# mi = mutual_info_regression(X.reshape(-1 , 1), y.reshape(-1,1))\n",
    "# float(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colleration using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.fc2 = nn.Linear(output_size  , 16)  # Fully connected layer\n",
    "        self.fc3 = nn.Linear(16  , 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)                  \n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)   \n",
    "        x = F.leaky_relu(x)     \n",
    "        x= self.fc3(x)\n",
    "        #x = torch.sigmoid(x)           \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_size = X.shape[1]    # Number of input features\n",
    "output_size = 1   # Single output probability\n",
    "model = SimpleNN(input_size, output_size).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train).cuda()\n",
    "X_test = torch.Tensor(X_test).cuda()\n",
    "y_train = torch.Tensor(y_train).cuda()\n",
    "y_test = torch.Tensor(y_test).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.2160\n",
      "Epoch [20/1000], Loss: 0.0008\n",
      "Epoch [30/1000], Loss: 0.5238\n",
      "Epoch [40/1000], Loss: 0.2471\n",
      "Epoch [50/1000], Loss: 0.4706\n",
      "Epoch [60/1000], Loss: 0.1405\n",
      "Epoch [70/1000], Loss: 0.2048\n",
      "Epoch [80/1000], Loss: 0.4937\n",
      "Epoch [90/1000], Loss: 0.0530\n",
      "Epoch [100/1000], Loss: 0.2320\n",
      "Epoch [110/1000], Loss: 0.3144\n",
      "Epoch [120/1000], Loss: 0.4421\n",
      "Epoch [130/1000], Loss: 0.1870\n",
      "Epoch [140/1000], Loss: 0.0777\n",
      "Epoch [150/1000], Loss: 0.2064\n",
      "Epoch [160/1000], Loss: 0.4179\n",
      "Epoch [170/1000], Loss: 0.3582\n",
      "Epoch [180/1000], Loss: 0.2702\n",
      "Epoch [190/1000], Loss: 0.1932\n",
      "Epoch [200/1000], Loss: 0.0308\n",
      "Epoch [210/1000], Loss: 0.0051\n",
      "Epoch [220/1000], Loss: 0.1963\n",
      "Epoch [230/1000], Loss: 0.1901\n",
      "Epoch [240/1000], Loss: 0.0699\n",
      "Epoch [250/1000], Loss: 0.4121\n",
      "Epoch [260/1000], Loss: 0.4510\n",
      "Epoch [270/1000], Loss: 0.0016\n",
      "Epoch [280/1000], Loss: 0.2146\n",
      "Epoch [290/1000], Loss: 0.0193\n",
      "Epoch [300/1000], Loss: 0.3103\n",
      "Epoch [310/1000], Loss: 0.1700\n",
      "Epoch [320/1000], Loss: 0.0718\n",
      "Epoch [330/1000], Loss: 0.0135\n",
      "Epoch [340/1000], Loss: 0.3209\n",
      "Epoch [350/1000], Loss: 0.2000\n",
      "Epoch [360/1000], Loss: 0.0125\n",
      "Epoch [370/1000], Loss: 0.0033\n",
      "Epoch [380/1000], Loss: 0.0178\n",
      "Epoch [390/1000], Loss: 0.0540\n",
      "Epoch [400/1000], Loss: 0.0281\n",
      "Epoch [410/1000], Loss: 0.1337\n",
      "Epoch [420/1000], Loss: 0.2518\n",
      "Epoch [430/1000], Loss: 0.2643\n",
      "Epoch [440/1000], Loss: 0.0977\n",
      "Epoch [450/1000], Loss: 0.5623\n",
      "Epoch [460/1000], Loss: 0.1775\n",
      "Epoch [470/1000], Loss: 0.3449\n",
      "Epoch [480/1000], Loss: 0.3103\n",
      "Epoch [490/1000], Loss: 0.0285\n",
      "Epoch [500/1000], Loss: 0.1918\n",
      "Epoch [510/1000], Loss: 0.0754\n",
      "Epoch [520/1000], Loss: 0.0028\n",
      "Epoch [530/1000], Loss: 0.3032\n",
      "Epoch [540/1000], Loss: 0.3165\n",
      "Epoch [550/1000], Loss: 0.3423\n",
      "Epoch [560/1000], Loss: 0.1335\n",
      "Epoch [570/1000], Loss: 0.2508\n",
      "Epoch [580/1000], Loss: 0.1978\n",
      "Epoch [590/1000], Loss: 0.0569\n",
      "Epoch [600/1000], Loss: 0.0093\n",
      "Epoch [610/1000], Loss: 0.3142\n",
      "Epoch [620/1000], Loss: 0.0531\n",
      "Epoch [630/1000], Loss: 0.3829\n",
      "Epoch [640/1000], Loss: 0.0000\n",
      "Epoch [650/1000], Loss: 0.1281\n",
      "Epoch [660/1000], Loss: 0.0093\n",
      "Epoch [670/1000], Loss: 0.0497\n",
      "Epoch [680/1000], Loss: 0.0028\n",
      "Epoch [690/1000], Loss: 0.0655\n",
      "Epoch [700/1000], Loss: 0.3253\n",
      "Epoch [710/1000], Loss: 0.0253\n",
      "Epoch [720/1000], Loss: 0.1464\n",
      "Epoch [730/1000], Loss: 0.2572\n",
      "Epoch [740/1000], Loss: 0.2124\n",
      "Epoch [750/1000], Loss: 0.0186\n",
      "Epoch [760/1000], Loss: 0.0694\n",
      "Epoch [770/1000], Loss: 0.0402\n",
      "Epoch [780/1000], Loss: 0.4263\n",
      "Epoch [790/1000], Loss: 0.0609\n",
      "Epoch [800/1000], Loss: 0.2486\n",
      "Epoch [810/1000], Loss: 0.2459\n",
      "Epoch [820/1000], Loss: 0.2570\n",
      "Epoch [830/1000], Loss: 0.0044\n",
      "Epoch [840/1000], Loss: 0.0022\n",
      "Epoch [850/1000], Loss: 0.0859\n",
      "Epoch [860/1000], Loss: 0.0209\n",
      "Epoch [870/1000], Loss: 0.0135\n",
      "Epoch [880/1000], Loss: 0.0090\n",
      "Epoch [890/1000], Loss: 0.0582\n",
      "Epoch [900/1000], Loss: 0.4177\n",
      "Epoch [910/1000], Loss: 0.0032\n",
      "Epoch [920/1000], Loss: 0.0056\n",
      "Epoch [930/1000], Loss: 0.2591\n",
      "Epoch [940/1000], Loss: 0.0057\n",
      "Epoch [950/1000], Loss: 0.0288\n",
      "Epoch [960/1000], Loss: 0.0018\n",
      "Epoch [970/1000], Loss: 0.0052\n",
      "Epoch [980/1000], Loss: 0.3411\n",
      "Epoch [990/1000], Loss: 0.0181\n",
      "Epoch [1000/1000], Loss: 0.3106\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:  # Print every 10 epochs\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28368449211120605"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, targets).item()\n",
    "\n",
    "# Calculate RMSE\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7795],\n",
       "        [0.7803],\n",
       "        [0.6474],\n",
       "        [0.8886],\n",
       "        [0.3525],\n",
       "        [0.5815],\n",
       "        [0.4591],\n",
       "        [0.6537],\n",
       "        [0.8709],\n",
       "        [0.4927]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(X_test)\n",
    "preds[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5406],\n",
       "        [0.7293],\n",
       "        [0.7958],\n",
       "        [0.7659],\n",
       "        [0.3943],\n",
       "        [0.1559],\n",
       "        [0.1700],\n",
       "        [0.8094],\n",
       "        [0.1829],\n",
       "        [0.1273]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3423, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt (( (preds - y_test) ** 2 ).mean() )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
